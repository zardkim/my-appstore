"""
í…ŒìŠ¤íŠ¸ ì œí’ˆì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì •í•˜ì—¬ ë” ì •í™•í•œ ì •ë³´ë¡œ ì—…ë°ì´íŠ¸
"""
import asyncio
from sqlalchemy.orm import Session
from app.database import SessionLocal
from app.models.product import Product
from app.core.web_crawler import WebCrawler
from app.core.ai_metadata import AIMetadataGenerator
from app.core.filename_standardizer import FilenameStandardizer
import html

async def fix_metadata():
    """í…ŒìŠ¤íŠ¸ ì œí’ˆì˜ ë©”íƒ€ë°ì´í„° ìˆ˜ì •"""
    db = SessionLocal()

    try:
        print("=" * 80)
        print("í…ŒìŠ¤íŠ¸ ì œí’ˆ ë©”íƒ€ë°ì´í„° ìˆ˜ì •")
        print("=" * 80)

        # ì›¹ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = WebCrawler()

        # í…ŒìŠ¤íŠ¸ í´ë”ì˜ ì œí’ˆë“¤ë§Œ ì„ íƒ
        products = db.query(Product).filter(
            Product.folder_path.like('/library/test_samples/%')
        ).all()

        print(f"\nìˆ˜ì •í•  ì œí’ˆ: {len(products)}ê°œ")

        for idx, product in enumerate(products, 1):
            print(f"\n{'=' * 80}")
            print(f"{idx}/{len(products)}: {product.title}")
            print("-" * 80)

            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_query = product.title

            # ì›¹ í¬ë¡¤ë§ ì‹¤í–‰
            print(f"ğŸ” ì›¹ í¬ë¡¤ë§: {search_query}")
            web_data = await crawler.search_web(search_query)

            if web_data:
                # ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸ (ìš°ì„ ìˆœìœ„: DuckDuckGo > ë‹¤ë¥¸ ì†ŒìŠ¤)
                if web_data.get('official_website'):
                    # FileHippoê°€ ì•„ë‹Œ ì‹¤ì œ ê³µì‹ ì‚¬ì´íŠ¸ ì°¾ê¸°
                    website = web_data.get('official_website', '')
                    if 'filehippo.com' not in website:
                        product.official_website = website
                        print(f"   âœ… ê³µì‹ ì›¹ì‚¬ì´íŠ¸: {website}")
                    else:
                        # additional_sourcesì—ì„œ ì°¾ê¸°
                        print(f"   âš ï¸  FileHippo ë°œê²¬, ë‹¤ë¥¸ ì†ŒìŠ¤ ì°¾ëŠ” ì¤‘...")

                # ìƒì„¸ ì„¤ëª… ë””ì½”ë”© ë° ì •ë¦¬
                if web_data.get('additional_info'):
                    raw_desc = web_data['additional_info']
                    # HTML ì—”í‹°í‹° ë””ì½”ë”©
                    decoded_desc = html.unescape(raw_desc)
                    # ì¤„ë°”ê¿ˆ ì •ë¦¬
                    lines = [line.strip() for line in decoded_desc.split('|') if line.strip()]
                    # FileHippo ë‹¤ìš´ë¡œë“œ í˜ì´ì§€ëŠ” ì œì™¸
                    filtered_lines = [
                        line for line in lines
                        if 'FileHippo ë‹¤ìš´ë¡œë“œ í˜ì´ì§€' not in line and len(line) > 20
                    ]
                    product.detailed_description = ' | '.join(filtered_lines)
                    print(f"   âœ… ìƒì„¸ ì„¤ëª… ì—…ë°ì´íŠ¸")

                # ìŠ¤í¬ë¦°ìƒ· ì—…ë°ì´íŠ¸
                if web_data.get('screenshots'):
                    # FileHippo ì¼ë°˜ ì•„ì´ì½˜ ì œê±°
                    screenshots = [
                        url for url in web_data['screenshots']
                        if 'cabb87.png' not in url  # Avast ì•„ì´ì½˜ ì œê±°
                    ]
                    product.screenshots = screenshots
                    print(f"   âœ… ìŠ¤í¬ë¦°ìƒ·: {len(screenshots)}ê°œ")

                    # ì²« ìŠ¤í¬ë¦°ìƒ·ì„ ì•„ì´ì½˜ìœ¼ë¡œ ì‚¬ìš© (ì¼ë°˜ ì•„ì´ì½˜ì´ ì•„ë‹Œ ê²½ìš°)
                    if screenshots and 'cabb87.png' in (product.icon_url or ''):
                        product.icon_url = screenshots[0]
                        print(f"   âœ… ì•„ì´ì½˜ ì—…ë°ì´íŠ¸: {screenshots[0][:60]}...")

                # í¬ë¡¤ë§ ì†ŒìŠ¤ ì—…ë°ì´íŠ¸
                if web_data.get('web_sources'):
                    product.crawled_from = {
                        source: True for source in web_data['web_sources']
                    }

            # AIë¡œ ë” ë‚˜ì€ ì„¤ëª… ìƒì„±
            print(f"ğŸ¤– AI ì„¤ëª… ìƒì„± ì¤‘...")
            try:
                ai_gen = AIMetadataGenerator(provider="gemini", model="gemini-2.5-flash")
                ai_data = await ai_gen.generate_metadata(product.title, "")

                # AI ì„¤ëª…ì´ ë” ë‚˜ì€ ê²½ìš° ì—…ë°ì´íŠ¸
                if ai_data.get('description') and len(ai_data['description']) > len(product.description or ''):
                    if 'software' not in ai_data['description'].lower():  # ë„ˆë¬´ ì¼ë°˜ì ì´ì§€ ì•Šì€ ê²½ìš°
                        product.description = ai_data['description']
                        print(f"   âœ… AI ì„¤ëª… ì—…ë°ì´íŠ¸: {ai_data['description'][:60]}...")
            except Exception as e:
                print(f"   âš ï¸  AI ìƒì„± ì‹¤íŒ¨: {e}")

            print(f"\n   í˜„ì¬ ìƒíƒœ:")
            print(f"   - ì œëª©: {product.title}")
            print(f"   - ì„¤ëª…: {(product.description or 'N/A')[:60]}...")
            print(f"   - ê³µì‹ ì‚¬ì´íŠ¸: {product.official_website or 'N/A'}")
            print(f"   - ì•„ì´ì½˜: {(product.icon_url or 'N/A')[:60]}...")
            print(f"   - ìŠ¤í¬ë¦°ìƒ·: {len(product.screenshots or [])}ê°œ")

            # ë”œë ˆì´
            if idx < len(products):
                await asyncio.sleep(3)

        # ì»¤ë°‹
        db.commit()

        print("\n" + "=" * 80)
        print("ë©”íƒ€ë°ì´í„° ìˆ˜ì • ì™„ë£Œ!")
        print("=" * 80)

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        db.rollback()
        import traceback
        traceback.print_exc()

    finally:
        db.close()

if __name__ == "__main__":
    asyncio.run(fix_metadata())
