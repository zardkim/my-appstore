"""
AI + ì›¹ í¬ë¡¤ë§ í†µí•© ë©”íƒ€ë°ì´í„° ìƒì„±ê¸°

ì „ëµ:
1. íŒŒì¼ëª… í‘œì¤€í™” ë° íŒŒì‹±
2. AI (Gemini/OpenAI): ê¸°ë³¸ ë©”íƒ€ë°ì´í„° (ì œëª©, ê°„ë‹¨í•œ ì„¤ëª…, ì œì¡°ì‚¬, ì¹´í…Œê³ ë¦¬, ì•„ì´ì½˜ URL)
3. ì›¹ í¬ë¡¤ë§: ìƒì„¸ ì •ë³´ (ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬ì–‘, ì£¼ìš” ê¸°ëŠ¥, ì§€ì› í¬ë§·, ë¦´ë¦¬ì¦ˆ ì •ë³´ ë“±)
"""
from typing import Dict, Optional
from datetime import datetime
from app.core.ai_metadata import AIMetadataGenerator
from app.core.web_crawler import WebCrawler
from app.core.filename_standardizer import FilenameStandardizer


class MetadataEnricher:
    """
    AIì™€ ì›¹ í¬ë¡¤ë§ì„ ê²°í•©í•œ í†µí•© ë©”íƒ€ë°ì´í„° ìƒì„±ê¸°
    """

    def __init__(
        self,
        ai_provider: str = "gemini",
        ai_model: str = "gemini-2.5-flash",
        use_ai: bool = True,
        use_web_crawling: bool = True
    ):
        """
        Args:
            ai_provider: AI ì œê³µì (gemini, openai, claude, azure)
            ai_model: AI ëª¨ë¸ëª…
            use_ai: AI ì‚¬ìš© ì—¬ë¶€
            use_web_crawling: ì›¹ í¬ë¡¤ë§ ì‚¬ìš© ì—¬ë¶€
        """
        self.use_ai = use_ai
        self.use_web_crawling = use_web_crawling

        # AI ë©”íƒ€ë°ì´í„° ìƒì„±ê¸°
        if self.use_ai:
            self.ai_generator = AIMetadataGenerator(
                provider=ai_provider,
                model=ai_model
            )
        else:
            self.ai_generator = None

        # ì›¹ í¬ë¡¤ëŸ¬
        if self.use_web_crawling:
            self.web_crawler = WebCrawler()
        else:
            self.web_crawler = None

    async def generate_metadata(
        self,
        filename: str,
        parent_folder: str = ""
    ) -> Dict:
        """
        AI + ì›¹ í¬ë¡¤ë§ì„ ê²°í•©í•˜ì—¬ ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ìƒì„±

        Args:
            filename: íŒŒì¼ëª…
            parent_folder: ë¶€ëª¨ í´ë”ëª…

        Returns:
            {
                # AI ìƒì„± ê¸°ë³¸ ì •ë³´
                'title': str,
                'description': str,
                'vendor': str,
                'category': str,
                'icon_url': str,

                # ì›¹ í¬ë¡¤ë§ ìƒì„¸ ì •ë³´
                'official_website': str,
                'license_type': str,
                'platform': str,
                'detailed_description': str,
                'features': List[str],
                'system_requirements': Dict,
                'supported_formats': Dict,
                'release_notes': str,
                'release_date': str,
                'crawled_from': Dict,
                'last_crawled_at': datetime
            }
        """
        metadata = {}

        # 1ë‹¨ê³„: AIë¡œ ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ìƒì„±
        if self.use_ai and self.ai_generator:
            print(f"ğŸ¤– AI ë©”íƒ€ë°ì´í„° ìƒì„± ì¤‘: {filename}")
            try:
                ai_metadata = await self.ai_generator.generate_metadata(
                    filename=filename,
                    parent_folder=parent_folder
                )
                metadata.update(ai_metadata)
                print(f"âœ… AI ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ: {metadata.get('title', 'Unknown')}")
            except Exception as e:
                print(f"âš ï¸ AI ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
                # AI ì‹¤íŒ¨ ì‹œ íŒŒì‹±ë§Œìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ ìƒì„±
                from app.core.parser import FilenameParser
                parser = FilenameParser()
                parsed = parser.parse(filename, parent_folder)
                metadata = {
                    'title': parsed.get('software_name', filename),
                    'description': f"{parsed.get('software_name', filename)} ì†Œí”„íŠ¸ì›¨ì–´",
                    'vendor': parsed.get('vendor', ''),
                    'category': 'Utility',
                    'icon_url': ''
                }

        # 2ë‹¨ê³„: ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        if self.use_web_crawling and self.web_crawler:
            # íŒŒì¼ëª… í‘œì¤€í™”ë¡œ ê¹¨ë—í•œ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_query = FilenameStandardizer.get_search_query(filename)

            print(f"ğŸŒ ì›¹ í¬ë¡¤ë§ ì‹œì‘: {search_query}")
            print(f"   (ì›ë³¸: {metadata.get('title', filename)})")

            try:
                # í‘œì¤€í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ì‚¬ìš©

                # ì›¹ í¬ë¡¤ë§ ì‹¤í–‰
                web_metadata = await self.web_crawler.search_web(search_query)

                # ì›¹ í¬ë¡¤ë§ ê²°ê³¼ ë³‘í•©
                if web_metadata:
                    # ê³µì‹ ì›¹ì‚¬ì´íŠ¸
                    if web_metadata.get('official_website'):
                        metadata['official_website'] = web_metadata['official_website']

                    # ìƒì„¸ ì„¤ëª… (ì›¹ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì´ ë” ìƒì„¸í•¨)
                    if web_metadata.get('additional_info'):
                        metadata['detailed_description'] = web_metadata['additional_info']

                    # ìŠ¤í¬ë¦°ìƒ· ìˆ˜ì§‘
                    if web_metadata.get('screenshots'):
                        metadata['screenshots'] = web_metadata['screenshots']
                        # ì²« ë²ˆì§¸ ìŠ¤í¬ë¦°ìƒ·ì„ ì•„ì´ì½˜ìœ¼ë¡œ í™œìš© (AIê°€ ì•„ì´ì½˜ì„ ëª» ì°¾ì€ ê²½ìš°)
                        if not metadata.get('icon_url'):
                            metadata['icon_url'] = web_metadata['screenshots'][0]

                    # ë‹¤ìš´ë¡œë“œ URL
                    if web_metadata.get('download_url'):
                        metadata['download_url'] = web_metadata['download_url']

                    # ì›¹ ì†ŒìŠ¤ ì •ë³´
                    if web_metadata.get('web_sources'):
                        metadata['crawled_from'] = {
                            source: True for source in web_metadata['web_sources']
                        }

                    print(f"âœ… ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {len(web_metadata.get('web_sources', []))}ê°œ ì†ŒìŠ¤")

            except Exception as e:
                print(f"âš ï¸ ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                metadata['crawled_from'] = {'error': str(e)}

        # í¬ë¡¤ë§ ì‹œê°„ ê¸°ë¡
        metadata['last_crawled_at'] = datetime.now()

        return metadata

    async def enrich_existing_metadata(
        self,
        existing_metadata: Dict,
        force_recrawl: bool = False
    ) -> Dict:
        """
        ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ë¥¼ ë³´ê°•

        Args:
            existing_metadata: ê¸°ì¡´ ë©”íƒ€ë°ì´í„°
            force_recrawl: ê°•ì œ ì¬í¬ë¡¤ë§ ì—¬ë¶€

        Returns:
            ë³´ê°•ëœ ë©”íƒ€ë°ì´í„°
        """
        # ê¸°ì¡´ ë°ì´í„°ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë³´ê°•
        enriched = existing_metadata.copy()

        # ì´ë¯¸ í¬ë¡¤ë§ëœ ê²½ìš° force_recrawlì´ ì•„ë‹ˆë©´ ìŠ¤í‚µ
        if not force_recrawl and existing_metadata.get('last_crawled_at'):
            print("ğŸ“¦ ì´ë¯¸ í¬ë¡¤ë§ëœ ë°ì´í„° ì¡´ì¬, ìŠ¤í‚µ")
            return enriched

        # ì›¹ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ë§Œ ì¶”ê°€
        if self.use_web_crawling and self.web_crawler:
            software_name = existing_metadata.get('title', '')
            vendor = existing_metadata.get('vendor', '')

            if software_name:
                search_query = f"{vendor} {software_name}".strip()
                web_metadata = await self.web_crawler.search_web(search_query)

                if web_metadata:
                    # ì—†ëŠ” ì •ë³´ë§Œ ì¶”ê°€ (ê¸°ì¡´ ì •ë³´ ìš°ì„ )
                    for key, value in web_metadata.items():
                        if key not in enriched or not enriched[key]:
                            enriched[key] = value

                    enriched['last_crawled_at'] = datetime.now()

        return enriched


# í¸ì˜ í•¨ìˆ˜
async def generate_full_metadata(
    filename: str,
    parent_folder: str = "",
    use_ai: bool = True,
    use_web: bool = True
) -> Dict:
    """
    ì™„ì „í•œ ë©”íƒ€ë°ì´í„° ìƒì„± (AI + ì›¹ í¬ë¡¤ë§)

    Args:
        filename: íŒŒì¼ëª…
        parent_folder: ë¶€ëª¨ í´ë”ëª…
        use_ai: AI ì‚¬ìš© ì—¬ë¶€
        use_web: ì›¹ í¬ë¡¤ë§ ì‚¬ìš© ì—¬ë¶€

    Returns:
        ì™„ì „í•œ ë©”íƒ€ë°ì´í„°
    """
    enricher = MetadataEnricher(
        ai_provider="gemini",
        ai_model="gemini-2.5-flash",
        use_ai=use_ai,
        use_web_crawling=use_web
    )

    return await enricher.generate_metadata(filename, parent_folder)
